{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-74seGUq09",
        "outputId": "dbaa60c8-0b7b-41d5-9b86-0e43c34df3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install libraries for both offline (Whisper) and online (Google) recognition\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import warnings\n",
        "\n",
        "# Suppress noisy warnings from the whisper library\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "audio_file_path = \"/content/lab3sample.wav\"\n",
        "\n",
        "#Offline Recognition with OpenAI's Whisper\n",
        "print(\"Starting Offline Recognition with Whisper\")\n",
        "try:\n",
        "    # Feedback: Let the user know recognition is in progress\n",
        "    print(\"Recognizing with Whisper...\")\n",
        "\n",
        "    # Load the base model (good balance of speed and accuracy)\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_file_path, fp16=False)\n",
        "\n",
        "    # Display the recognized text\n",
        "    recognized_text = result[\"text\"].strip()\n",
        "    print(f\"Speech recognized: '{recognized_text}'\")\n",
        "\n",
        "    # Display success message\n",
        "    print(\"Speech successfully converted to text!\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Handle any other exceptions during Whisper processing\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator for clarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_B-o4P8Vxoz",
        "outputId": "b8c38182-5353-4466-d757-f6209d84d004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Offline Recognition with Whisper\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 41.6MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech recognized: 'I believe you're just talking nonsense.'\n",
            "Speech successfully converted to text!\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Online Recognition with Google Speech API\n",
        "print(\"--- Starting Online Recognition with Google Speech API ---\")\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Use the audio file as the source\n",
        "with sr.AudioFile(audio_file_path) as source:\n",
        "    # Feedback: Let the user know what to do (though we are using a file)\n",
        "    print(\"Speak something... (using audio file as source)\")\n",
        "\n",
        "    # Read the audio data from the file\n",
        "    audio_data = r.record(source)\n",
        "\n",
        "    # Feedback: Let the user know recognition is in progress\n",
        "    print(\"Recognizing with Google API...\")\n",
        "\n",
        "    # Try to recognize the speech using Google's free Web Speech API\n",
        "    try:\n",
        "        # Convert speech to text\n",
        "        recognized_text = r.recognize_google(audio_data)\n",
        "\n",
        "        # Display the recognized text\n",
        "        print(f\"Speech recognized: '{recognized_text}'\")\n",
        "\n",
        "        # Display success message\n",
        "        print(\"Speech successfully converted to text!\")\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "        # Handle unclear speech\n",
        "        print(\"Speech Recognition could not understand audio. Please try speaking more clearly.\")\n",
        "    except sr.RequestError as e:\n",
        "        # Handle service unavailability\n",
        "        print(f\"Could not request results from Google Speech Recognition service; {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB41YEIlWsEr",
        "outputId": "b854e126-a2cc-4cca-dc36-9eb337b8c837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Online Recognition with Google Speech API ---\n",
            "Speak something... (using audio file as source)\n",
            "Recognizing with Google API...\n",
            "Speech recognized: 'I believe you are just talking nonsense'\n",
            "Speech successfully converted to text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMTCPGxJi8vE",
        "outputId": "bd47858e-8e28-490e-fa85-eaba04538851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Female Voice"
      ],
      "metadata": {
        "id": "DAOGf8Vx-uQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import warnings\n",
        "import os\n",
        "import vosk\n",
        "import wave\n",
        "import json\n",
        "\n",
        "audio_file_path = \"/content/voice.wav\"\n",
        "\n",
        "# OFFLINE WITH WHISPER\n",
        "print(\"Transcribing with Whisper (Offline Model)\")\n",
        "try:\n",
        "    # Load the 'base' Whisper model\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_file_path, fp16=False)\n",
        "\n",
        "    # Print the recognized text\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Visual separator\n",
        "\n",
        "# ONLINE WITH GOOGLE SPEECH API\n",
        "print(\" Transcribing with Google Speech API (Online Model) \")\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Process the audio file\n",
        "with sr.AudioFile(audio_file_path) as source:\n",
        "    # Read the audio data from the file\n",
        "    audio_data = r.record(source)\n",
        "\n",
        "    # Try to recognize the speech using Google's API\n",
        "    try:\n",
        "        # Send audio to Google for transcription\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "\n",
        "        # Print the recognized text\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "        # This error happens if the API can't understand the audio\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        # This error happens if there's a problem with the network or the API service\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "# OFFLINE WITH VOSK\n",
        "print(\" Transcribing with Vosk (Offline Model) \")\n",
        "try:\n",
        "    # Check for and download the Vosk model\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        # Using shell commands which are simple and effective in Colab\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    # Load the Vosk model\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "\n",
        "    # Open the audio file in the format Vosk requires\n",
        "    wf = wave.open(audio_file_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    # Process the audio data in chunks\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    # Get the final result and parse the JSON output\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvzUYePIgNFB",
        "outputId": "9f02157d-c44b-4dcd-8cca-e849d3a00d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing with Whisper (Offline Model)\n",
            "Whisper Output: 'Hi there. How are you doing? Take care. Be happy. Bye.'\n",
            "\n",
            "==================================================\n",
            "\n",
            " Transcribing with Google Speech API (Online Model) \n",
            "Google API Output: 'hi there how are you doing take care be happy bye'\n",
            " Transcribing with Vosk (Offline Model) \n",
            "Vosk model not found. Downloading 'vosk-model-small-en-us-0.15'...\n",
            "Model downloaded and unzipped successfully.\n",
            "Vosk Output: 'on him home one to to be oh boy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Male voice"
      ],
      "metadata": {
        "id": "ytEXU0hQ_CsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk pydub\n",
        "!apt-get install -y -qq ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5B1AE08_EoB",
        "outputId": "8a0e7ccf-74e1-45b9-ac27-d955f67111f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import vosk\n",
        "import warnings\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "original_audio_path = \"/content/male Voice.wav\"\n",
        "converted_audio_path = \"converted_audio_standard.wav\"\n",
        "\n",
        "#AUDIO CONVERSION STEP (IMPROVED)\n",
        "print(\"--- Converting audio file to a standard format for compatibility ---\")\n",
        "try:\n",
        "    sound = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "    sound = sound.set_channels(1) # Mono\n",
        "    sound = sound.set_frame_rate(16000) # 16kHz sample rate\n",
        "    sound.export(converted_audio_path, format=\"wav\")\n",
        "    print(\"Conversion successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Audio conversion failed: {e}\")\n",
        "    converted_audio_path = original_audio_path\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# - OFFLINE WITH WHISPER\n",
        "print(\"--- 1. Transcribing with Whisper (Offline Model) ---\")\n",
        "try:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(original_audio_path, fp16=False)\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  ONLINE WITH GOOGLE SPEECH API\n",
        "print(\"--- 2. Transcribing with Google Speech API (Online Model) ---\")\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(converted_audio_path) as source:\n",
        "    audio_data = r.record(source)\n",
        "    try:\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH VOSK\n",
        "print(\"--- 3. Transcribing with Vosk (Offline Model) ---\")\n",
        "try:\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "    # Use the new, standardized audio file\n",
        "    wf = wave.open(converted_audio_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nh1jrxVAGZ3",
        "outputId": "8030320f-30a1-4fb1-ba81-f3d21135032f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting audio file to a standard format for compatibility ---\n",
            "Conversion successful.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 1. Transcribing with Whisper (Offline Model) ---\n",
            "Whisper Output: 'Hi, hello, can I talk with Manasa? Is everything okay? Have a nice day, take care.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 2. Transcribing with Google Speech API (Online Model) ---\n",
            "Google API Output: 'hi hello can I talk with Mansa is everything okay have a nice day take care'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 3. Transcribing with Vosk (Offline Model) ---\n",
            "Vosk Output: 'hi hello can it don't commit an answer is everything okay i'm in a day daycare'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast speech"
      ],
      "metadata": {
        "id": "T5nNO4baAvRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk pydub\n",
        "!apt-get install -y -qq ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "151T8nh2Ay1P",
        "outputId": "4575fcbb-6993-4244-c6d8-8c4e1b34f284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import vosk\n",
        "import warnings\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "original_audio_path = \"/content/fast Voice.wav\"\n",
        "converted_audio_path = \"converted_audio_standard.wav\"\n",
        "\n",
        "# --- AUDIO CONVERSION STEP\n",
        "print(\"--- Converting audio file to a standard format for compatibility ---\")\n",
        "try:\n",
        "    sound = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "    sound = sound.set_channels(1) # Mono\n",
        "    sound = sound.set_frame_rate(16000) # 16kHz sample rate\n",
        "    sound.export(converted_audio_path, format=\"wav\")\n",
        "    print(\"Conversion successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Audio conversion failed: {e}\")\n",
        "    converted_audio_path = original_audio_path\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# OFFLINE WITH WHISPER\n",
        "print(\"--- 1. Transcribing with Whisper (Offline Model) ---\")\n",
        "try:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(original_audio_path, fp16=False)\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# ONLINE WITH GOOGLE SPEECH API\n",
        "print(\"--- 2. Transcribing with Google Speech API (Online Model) ---\")\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(converted_audio_path) as source:\n",
        "    audio_data = r.record(source)\n",
        "    try:\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH VOSK\n",
        "print(\"--- 3. Transcribing with Vosk (Offline Model) ---\")\n",
        "try:\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "\n",
        "    wf = wave.open(converted_audio_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKRUXb0VD6Xz",
        "outputId": "755f11de-64c0-4045-b2b9-2e64987808d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting audio file to a standard format for compatibility ---\n",
            "Conversion successful.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 1. Transcribing with Whisper (Offline Model) ---\n",
            "Whisper Output: 'Hi everyone how are you doing? Take care. Bye.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 2. Transcribing with Google Speech API (Online Model) ---\n",
            "Google API Output: 'hi everyone how are you how are you doing take care bye'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 3. Transcribing with Vosk (Offline Model) ---\n",
            "Vosk Output: 'tyree when how are you are you doing they can buy'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise Background"
      ],
      "metadata": {
        "id": "7UDPYLp_E0iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git SpeechRecognition vosk pydub\n",
        "!apt-get install -y -qq ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnBwClMJE3Mk",
        "outputId": "dfab54d0-b91b-4b54-c677-6064e52875d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import vosk\n",
        "import warnings\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "original_audio_path = \"/content/Noisy Voice.wav\"\n",
        "converted_audio_path = \"converted_audio_standard.wav\"\n",
        "\n",
        "#  AUDIO CONVERSION STEP (IMPROVED)\n",
        "print(\"--- Converting audio file to a standard format for compatibility ---\")\n",
        "try:\n",
        "    sound = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "    sound = sound.set_channels(1) # Mono\n",
        "    sound = sound.set_frame_rate(16000) # 16kHz sample rate\n",
        "    sound.export(converted_audio_path, format=\"wav\")\n",
        "    print(\"Conversion successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Audio conversion failed: {e}\")\n",
        "    converted_audio_path = original_audio_path\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH WHISPER\n",
        "print(\"--- 1. Transcribing with Whisper (Offline Model) ---\")\n",
        "try:\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(original_audio_path, fp16=False)\n",
        "    print(f\"Whisper Output: '{result['text'].strip()}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Whisper: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  ONLINE WITH GOOGLE SPEECH API\n",
        "print(\"--- 2. Transcribing with Google Speech API (Online Model) ---\")\n",
        "r = sr.Recognizer()\n",
        "with sr.AudioFile(converted_audio_path) as source:\n",
        "    audio_data = r.record(source)\n",
        "    try:\n",
        "        google_text = r.recognize_google(audio_data)\n",
        "        print(f\"Google API Output: '{google_text}'\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Speech Recognition could not understand the audio.\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google service; {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "#  OFFLINE WITH VOSK\n",
        "print(\"--- 3. Transcribing with Vosk (Offline Model) ---\")\n",
        "try:\n",
        "    model_name = \"vosk-model-small-en-us-0.15\"\n",
        "    model_path = model_name\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Vosk model not found. Downloading '{model_name}'...\")\n",
        "        !wget -q https://alphacephei.com/vosk/models/{model_name}.zip\n",
        "        !unzip -q {model_name}.zip\n",
        "        print(\"Model downloaded and unzipped successfully.\")\n",
        "\n",
        "    vosk_model = vosk.Model(model_path)\n",
        "    # Use the new, standardized audio file\n",
        "    wf = wave.open(converted_audio_path, \"rb\")\n",
        "    rec = vosk.KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "\n",
        "    result_json = rec.FinalResult()\n",
        "    result_dict = json.loads(result_json)\n",
        "    vosk_text = result_dict['text']\n",
        "    print(f\"Vosk Output: '{vosk_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred with Vosk: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KDzVCNoE9PY",
        "outputId": "7cd7f818-e2b3-4915-dc90-f1d086ddcfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting audio file to a standard format for compatibility ---\n",
            "Conversion successful.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 1. Transcribing with Whisper (Offline Model) ---\n",
            "Whisper Output: 'Hello everyone, good morning, welcome to my YouTube channel. Now I am doing the lab 3x series of SPR that speaks to text application. Thank you.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 2. Transcribing with Google Speech API (Online Model) ---\n",
            "Google API Output: 'tell everyone good morning welcome to my YouTube channel now I'm doing the lab 3x images of SBR that speech to text application thank you'\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- 3. Transcribing with Vosk (Offline Model) ---\n",
            "Vosk Output: 'hello everyone that money will come to my you tube channel know i'm doing the lab three acres of and be that it's been to fix to application thank you'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "Google Speech API generally outperforms Whisper/Vosk on clear speech due to cloud-based large models.\n",
        "\n",
        "Whisper (offline) is robust in noisy environments but requires more computation.\n",
        "\n",
        "Vosk is lightweight and works offline but struggles with fast or soft speech.\n",
        "\n",
        "Error Handling:\n",
        "\n",
        "Meaningful error messages like “Could not understand audio, please try again” improve usability.\n",
        "\n",
        "Offline models (Whisper/Vosk) avoid internet issues, but Google API may fail when network is unavailable.\n",
        "\n",
        "Best Performing Method:\n",
        "\n",
        "Google Speech API → Most accurate for clear voices.\n",
        "\n",
        "Whisper → Best for noisy background or offline usage.\n",
        "\n",
        "Vosk → Lightweight but least accurate in challenging conditions.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "This project demonstrates that speech-to-text systems can significantly enhance accessibility by converting spoken commands into text in real time. Among the tested methods, Google Speech API delivered the highest accuracy for clear speech, while Whisper showed resilience in noisy environments and Vosk provided a lightweight offline option. However, challenges remain with soft voices and very fast speech.\n",
        "\n",
        "Future Improvements:\n",
        "\n",
        "Integrate noise reduction preprocessing before recognition.\n",
        "\n",
        "Add multi-language support.\n",
        "\n",
        "Extend to real-time device control (smart home / accessibility apps).\n",
        "\n",
        "Improve user interface with visual + audio feedback."
      ],
      "metadata": {
        "id": "rtyT_2FaNl8M"
      }
    }
  ]
}